{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d04dc5e-a013-4c62-88fe-65f95c477c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_file, read_image\n",
    "from typing import Union\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137c6cfb-0bd9-474b-a298-990c3213c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(xml_file: str):\n",
    "\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    labels = []\n",
    "    bboxes = []\n",
    "\n",
    "    for boxes in root.iter('object'):\n",
    "\n",
    "        label = boxes.find('name').text\n",
    "        labels.append(label)\n",
    "\n",
    "        ymin, xmin, ymax, xmax = None, None, None, None\n",
    "\n",
    "        ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
    "\n",
    "        bbox = [xmin, ymin, xmax, ymax]\n",
    "        bboxes.append(bbox)\n",
    "\n",
    "    return labels, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d724d6f-4404-4beb-9987-38c2b8e76bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelTransform():\n",
    "    def __init__(self, classes : list):\n",
    "        assert len(classes)>0, \"Number of classes should not be empty\"\n",
    "        self.labels = [\"__background__\"] + classes\n",
    "        self.index2labels = {l:idx for idx,l in enumerate(self.labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)-1\n",
    "\n",
    "    def __call__(self, labels: list):\n",
    "        return [self.index2labels[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "225a0b97-1bb0-4c75-b45f-a04e15e49ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDTransform():\n",
    "    '''\n",
    "    Custom transformation class that does one of the following tranformation in uniform distribution:\n",
    "        - No transformation\n",
    "        - ColorJitter\n",
    "        - HorizontalFlip\n",
    "        - Rotation (clockwise 90 degrees)\n",
    "        - Rotation (Anti clockwise 90 degrees)\n",
    "    '''\n",
    "\n",
    "    TRANSFORMS = [\"horizontal_flip\", \"color_jitter\", \"clockwise_rotate\", \"anitclockwise_rotate\"]\n",
    "    \n",
    "    def __init__(self, training: bool = False, hflip : float = False, rotate_1 : Union[int, tuple] = None, rotate_2: Union[int, tuple] = None):\n",
    "        self.training = training\n",
    "        self.color_jitter = v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "        self.hflip = hflip\n",
    "        if hflip:\n",
    "            self.horizontal_flip = v2.RandomHorizontalFlip(p=1.0)\n",
    "        self.clockwise_rotate = rotate_1\n",
    "        if rotate_1 is not None:\n",
    "            self.random_rotate_clockwise = v2.RandomRotation(rotate_1)\n",
    "        self.anticlockwise_rotate = rotate_2\n",
    "        if rotate_2 is not None:\n",
    "            self.random_rotate_anticlockwise = v2.RandomRotation(rotate_2)\n",
    "\n",
    "        self.intervals = list(np.arange(len(self.TRANSFORMS)+1) * (1/(len(self.TRANSFORMS)+1))) + [1.0]\n",
    "\n",
    "        self.default_transforms = v2.Compose(\n",
    "            [v2.ToImage(), v2.ToDtype(dtype=torch.float32, scale=True)]\n",
    "        )\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            p = random.random()\n",
    "    \n",
    "            for idx, (li,ri) in enumerate(zip(self.intervals[:-1],self.intervals[1:])):\n",
    "                if p>li and p<=ri:\n",
    "                    if idx == 0:\n",
    "                        break\n",
    "                    if idx == 1:\n",
    "                        # color jitter tranformation\n",
    "                        image, targets = self.color_jitter(image, targets)\n",
    "                    if idx == 2:\n",
    "                        # clockwise rotation tranformation\n",
    "                        image, targets = self.random_rotate_clockwise(image, targets)\n",
    "                    if idx == 3:\n",
    "                        # anitclockwise rotation tranformation\n",
    "                        image, targets = self.random_rotate_anticlockwise(image, targets)\n",
    "                    if idx == 4:\n",
    "                        # horzontal flip\n",
    "                        image, targets = self.horizontal_flip(image, targets)\n",
    "\n",
    "        image, targets = self.default_transforms(image, targets)\n",
    "        return image, targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9ba18ff-1acf-4f8e-9a95-ea4c38b10012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDDataset(Dataset):\n",
    "    def __init__(self, img_folder: str, label_transform: LabelTransform = None, transform: SSDTransform = None):\n",
    "        \n",
    "        assert label_transform is not None, \"Label transform should not be None\"\n",
    "        \n",
    "        self.img_paths = [ os.path.join(img_folder,filepath) for filepath in os.listdir(img_folder) if not filepath.endswith(\".xml\")]\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        xml_path = img_path.replace(os.path.splitext(img_path)[1], \".xml\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        W,H = image.size\n",
    "        \n",
    "        labels,bboxes = [], []\n",
    "        if os.path.exists(xml_path):\n",
    "            labels, bboxes = parse_xml(xml_path)\n",
    "\n",
    "        target = {}\n",
    "        \n",
    "        if self.label_transform is not None and labels:\n",
    "            labels = self.label_transform(labels)\n",
    "\n",
    "        labels = torch.as_tensor(labels)\n",
    "            \n",
    "        target[\"boxes\"] =  tv_tensors.BoundingBoxes(torch.as_tensor(bboxes), format=\"XYXY\", canvas_size=(H,W))\n",
    "        target[\"labels\"] = labels\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14b1826a-fbdf-4345-9eb9-aa5ed93a1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return list(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450c806-d74c-4a25-9d2b-1d9dd9cd84ff",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d7a4f-d8b6-4b4d-8978-59f8988f9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "model = torchvision.models.detection.ssd300_vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b98ba79-b63d-4960-9abe-4a59778b7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"signature\", \"people\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e4fd56c-87fb-4dfc-8f81-44fcad4adcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = \"D:/00_Projects/ImageProcessing/ssd/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e843413-de9d-4898-b1c5-cdc0a68f92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transform = LabelTransform(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80aca120-a94d-42cd-8f86-29ffcbca8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SSDTransform(hflip=True, rotate_1=(0,10), rotate_2=(-10,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ed96bb7-f54b-444b-a66c-4a99ac4c7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SSDDataset(train_img_dir,label_transform, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "338b4ba1-2fd8-42e8-8f05-68a898d7963f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f34895e-cc0d-4dd9-a483-493212a34f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset=train_ds, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8871dd81-412c-4f57-a549-2ee58362fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr= 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "726783b2-8f99-45d7-8fc0-965487649d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ae56ec9-14ad-4e9d-aea5-277b1f525d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be70fd3a8a34313b126362d51a5bf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c3abedb291471db40d0d5ae241a7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879d5814efa24a249a9cb03db9af6e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720a9a0bc35348b5835bd1fcc87d2467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e405db85ea42b385de1f18a6c52a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2c35f4a8f74074ad4455ff83d6a1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "etl = tnrange(EPOCHS)\n",
    "for epoch in etl:\n",
    "    epoch_trn_losses = []\n",
    "    tdl = tqdm(train_dl, total=len(train_dl), leave = True)\n",
    "    for batch in tdl:\n",
    "        images, targets = batch\n",
    "        losses = model(images, targets)\n",
    "        loss = (losses['bbox_regression'] + losses['classification'])\n",
    "        tdl.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_trn_losses.append(loss.item())\n",
    "\n",
    "    epoch_trn_loss = np.array(epoch_trn_losses).mean()\n",
    "    etl.set_postfix(epoch_loss = epoch_trn_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9415d2-3734-4e71-8828-ee140dadcd50",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cb63b31-094f-4ded-a29c-5f68f949eda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD(\n",
       "  (backbone): SSDFeatureExtractorVGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "    )\n",
       "    (extra): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3-4): 2 x Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05], steps=[8, 16, 32, 64, 100, 300])\n",
       "  (head): SSDHead(\n",
       "    (classification_head): SSDClassificationHead(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Conv2d(512, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(1024, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(512, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4-5): 2 x Conv2d(256, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (regression_head): SSDRegressionHead(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n",
       "      Resize(min_size=(300,), max_size=300, mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6a60af6-4b2a-4495-bd6f-669a619f4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_dir = \"D:/00_Projects/ImageProcessing/ssd/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "55ae0b46-2312-4e8b-ba32-406508e50d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = SSDTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17ac69a2-f9e3-4636-8b00-c6952a940680",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = SSDDataset(test_img_dir,label_transform, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0af4513f-83ad-4185-b32f-c74dcafbf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(dataset=test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62a5e8cc-05ab-4c64-bae5-ce3ca9b90c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in test_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46337929-48b7-4626-8d4c-a76481a3354f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[ 24.0123, 190.4505, 296.4680, 396.7329],\n",
       "          [410.1459,  42.3025, 611.8274, 148.4481],\n",
       "          [445.9295,  69.1654, 646.8311, 175.2687],\n",
       "          [386.7603, 194.3256, 659.3284, 399.4393],\n",
       "          [410.4062,  95.9208, 614.2113, 203.8828],\n",
       "          [241.3191,  55.5308, 525.8620, 204.5639],\n",
       "          [  0.0000,  50.1355, 265.2478, 229.2480],\n",
       "          [385.6357,  41.2019, 660.7468, 247.0745],\n",
       "          [330.4872,   6.1896, 582.2155, 179.6060],\n",
       "          [249.3162, 195.1672, 523.9681, 398.5835],\n",
       "          [171.2823,   7.0833, 454.6725, 155.8632],\n",
       "          [ 26.7208, 106.3832, 294.6958, 283.6154],\n",
       "          [ 24.9710,   2.9666, 303.8183, 156.8428],\n",
       "          [ 96.4392,  56.1436, 381.3443, 204.8586],\n",
       "          [180.9019, 246.0641, 458.5162, 426.1462],\n",
       "          [171.1024, 111.2884, 461.0391, 258.0874],\n",
       "          [172.8290, 167.7478, 464.7236, 314.8942],\n",
       "          [318.7914, 111.3342, 601.7883, 256.7699],\n",
       "          [431.6300, 167.0632, 664.0000, 314.5848],\n",
       "          [151.6027,  10.0498, 428.0085, 476.4867],\n",
       "          [371.7780,  64.8471, 583.6357, 120.8330],\n",
       "          [372.0727,  91.7812, 583.5206, 147.7214],\n",
       "          [326.5225, 250.4540, 595.6652, 427.7441],\n",
       "          [329.3563, 163.0657, 590.3207, 338.5808],\n",
       "          [ 99.1655, 211.4430, 377.6235, 364.9604],\n",
       "          [378.6178, 276.4465, 580.4178, 332.7424],\n",
       "          [191.5929,  60.4710, 405.4032, 119.3142],\n",
       "          [192.6211,  87.4552, 404.3197, 146.2151],\n",
       "          [300.6621,  87.6204, 508.0107, 146.6525],\n",
       "          [362.4876,  31.8204, 585.2726,  92.1473],\n",
       "          [449.0963, 249.9923, 647.0238, 306.5424],\n",
       "          [450.8214, 281.5187, 649.4013, 332.0042],\n",
       "          [371.4116, 114.4315, 581.2717, 174.3484],\n",
       "          [447.5460, 223.0714, 648.2562, 279.7898],\n",
       "          [458.5952,  16.9815, 664.0000, 123.5266],\n",
       "          [232.2701, 275.6292, 442.3828, 332.5324],\n",
       "          [ 22.9441, 276.5152, 220.8892, 334.2513],\n",
       "          [ 67.5808, 260.9208, 254.0414, 371.1573],\n",
       "          [449.4998, 287.0244, 644.3079, 391.2709],\n",
       "          [ 56.9668, 249.5973, 257.5231, 307.4721],\n",
       "          [444.7794, 195.7303, 652.1369, 252.5143],\n",
       "          [445.8859, 122.0431, 650.4364, 232.0399],\n",
       "          [ 55.8314, 222.6363, 258.6538, 280.6195],\n",
       "          [189.9734, 113.7819, 410.8228, 174.1529],\n",
       "          [ 24.0986,  71.7501, 224.0537, 177.7969],\n",
       "          [229.3838, 248.5360, 445.6224, 305.6476],\n",
       "          [412.0090, 306.6530, 618.3212, 364.9684],\n",
       "          [253.2389,  31.4979, 484.2574,  91.1358],\n",
       "          [ 30.5178, 207.4255, 220.5501, 317.2296],\n",
       "          [ 91.9033, 280.2161, 297.4897, 332.3513],\n",
       "          [419.4675, 233.1475, 609.5034, 340.5559],\n",
       "          [ 53.7001, 195.2815, 261.7534, 253.2586],\n",
       "          [376.5633, 309.0430, 650.5493, 481.2823],\n",
       "          [337.9020, 248.6338, 552.2892, 305.7435],\n",
       "          [ 23.5234, 288.3103, 219.8931, 392.3407],\n",
       "          [123.8760,  46.1761, 325.5693, 153.1800],\n",
       "          [369.5420, 141.0874, 587.3942, 200.0875],\n",
       "          [227.4896, 221.4425, 447.6410, 278.8994],\n",
       "          [434.2714, 362.7100, 658.0488, 454.0196],\n",
       "          [369.6364, 168.0227, 591.1202, 225.1318],\n",
       "          [270.1028,  87.3370, 513.2679, 309.0162],\n",
       "          [373.7622, 315.4972, 588.3739, 407.9667],\n",
       "          [222.2366,  76.3319, 511.8360, 394.9839],\n",
       "          [345.0366,  47.6339, 651.4274, 391.4129],\n",
       "          [ 43.8324,  34.5175, 353.9735, 364.7132],\n",
       "          [504.4781,  69.7913, 664.0000, 413.3459],\n",
       "          [164.0060, 209.2637, 502.4480, 504.0000],\n",
       "          [  0.0000,  27.7597, 201.7640, 395.2818],\n",
       "          [  0.0000, 207.1743, 281.8996, 504.0000],\n",
       "          [226.0837,   0.0000, 503.5908, 196.8057],\n",
       "          [364.2890, 201.6250, 664.0000, 484.7014],\n",
       "          [ 88.6433,   0.0000, 373.7267, 194.4670],\n",
       "          [370.6713,   0.0000, 649.6069, 192.9259],\n",
       "          [514.4913,   0.0000, 664.0000, 230.1104],\n",
       "          [111.7505, 193.0260, 383.4780, 396.0831],\n",
       "          [  0.0000,   0.0000, 211.2901, 198.6488],\n",
       "          [523.4202, 221.3536, 664.0000, 504.0000],\n",
       "          [348.6447, 358.8328, 655.6920, 504.0000],\n",
       "          [205.7789, 370.7653, 509.1403, 504.0000],\n",
       "          [556.7376,   0.0000, 664.0000, 128.6202],\n",
       "          [  5.1691, 159.2478, 288.6495, 359.2248],\n",
       "          [566.0730, 100.0014, 664.0000, 301.0604],\n",
       "          [ 28.9910,   0.0000, 307.1697, 127.2478],\n",
       "          [ 63.0816, 356.4290, 386.2641, 504.0000],\n",
       "          [574.1837, 202.2919, 664.0000, 402.2462]], grad_fn=<StackBackward0>),\n",
       "  'scores': tensor([0.3394, 0.3273, 0.3153, 0.3015, 0.2728, 0.2679, 0.2589, 0.2575, 0.2546,\n",
       "          0.2538, 0.2422, 0.2400, 0.2385, 0.2381, 0.2272, 0.2185, 0.2183, 0.2123,\n",
       "          0.2122, 0.2089, 0.2044, 0.2023, 0.2018, 0.2008, 0.1982, 0.1969, 0.1925,\n",
       "          0.1912, 0.1858, 0.1845, 0.1837, 0.1812, 0.1775, 0.1759, 0.1724, 0.1718,\n",
       "          0.1710, 0.1689, 0.1679, 0.1655, 0.1642, 0.1614, 0.1613, 0.1594, 0.1575,\n",
       "          0.1559, 0.1549, 0.1546, 0.1545, 0.1518, 0.1514, 0.1499, 0.1483, 0.1472,\n",
       "          0.1468, 0.1437, 0.1436, 0.1436, 0.1423, 0.1411, 0.1408, 0.1407, 0.1107,\n",
       "          0.1102, 0.0852, 0.0635, 0.0617, 0.0427, 0.0398, 0.0340, 0.0332, 0.0310,\n",
       "          0.0299, 0.0255, 0.0238, 0.0196, 0.0181, 0.0164, 0.0136, 0.0134, 0.0134,\n",
       "          0.0134, 0.0132, 0.0131, 0.0129], grad_fn=<IndexBackward0>),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c21ab689-4d77-45a2-9df4-16bb6b01a2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'signature'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_transform.labels[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
